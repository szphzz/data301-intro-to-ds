{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Copy of 10.1 Bag of Words and N-Grams.ipynb","provenance":[{"file_id":"https://github.com/dlsun/pods/blob/master/10-Textual-Data/10.1%20Bag%20of%20Words%20and%20N-Grams.ipynb","timestamp":1651605499118}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-k5RX60yP9hv"},"source":["# Chapter 10 Textual Data\n","\n","You may not naturally think of _text_, like an e-mail or a newspaper article, as data. But just as we might predict the price of a home or cluster wines into similar types, we might want to predict the sender of an e-mail or cluster articles into similar types. To leverage the machine learning techniques from Part II of this book, we will need to convert raw text into tabular form. This chapter introduces some principles for doing so."]},{"cell_type":"markdown","metadata":{"id":"weXLO3u5P9hy"},"source":["# 10.1 Bag of Words and N-Grams\n","\n","In data science, a unit of text is typically called a _document_, even though a document can be anything from a text message to a full-length novel.  A collection of documents is called a _corpus_. In this lesson, we will work with a corpus of Dr. Seuss books."]},{"cell_type":"code","metadata":{"id":"EVHxasNCP9hz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651605798745,"user_tz":420,"elapsed":1207,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"c046f1a7-c952-48cf-e861-1a50ee4c8034"},"source":["import pandas as pd\n","import requests\n","\n","seuss_dir = \"http://dlsun.github.io/pods/data/drseuss/\"\n","seuss_files = [\n","    \"green_eggs_and_ham.txt\", \"cat_in_the_hat.txt\", \"fox_in_socks.txt\",\n","    \"hop_on_pop.txt\", \"horton_hears_a_who.txt\", \"how_the_grinch_stole_christmas.txt\",\n","    \"oh_the_places_youll_go.txt\", \"one_fish_two_fish.txt\"\n","]\n","\n","docs_seuss = pd.Series()\n","for file in seuss_files:\n","    response = requests.get(seuss_dir + file, \"r\")\n","    docs_seuss[file[:-4]] = response.text\n","\n","docs_seuss"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                I am Sam\\n\\nI am Sam\\nSam I am\\n\\nThat Sam-I-a...\n","cat_in_the_hat                    The sun did not shine.\\nIt was too wet to play...\n","fox_in_socks                      Fox\\nSocks\\nBox\\nKnox\\n\\nKnox in box.\\nFox in ...\n","hop_on_pop                        UP PUP Pup is up.\\nCUP PUP Pup in cup.\\nPUP CU...\n","horton_hears_a_who                On the fifteenth of May, in the jungle of Nool...\n","how_the_grinch_stole_christmas    Every Who\\nDown in Whoville\\nLiked Christmas a...\n","oh_the_places_youll_go            Congratulations!\\nToday is your day.\\nYou're o...\n","one_fish_two_fish                 One fish, two fish, red fish, blue fish,\\nBlac...\n","dtype: object"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"g_-_HcWAP9h3"},"source":["Suppose, for example, that we want to determine which two Dr. Seuss books are most similar or cluster the books into several types. In order to leverage the methods that we learned in Parts I and II of this book, we have to convert these documents into tabular form. In this lesson, we focus on a particular representation of a document called the _bag of words_.\n","\n","The _bag of words_ representation reduces a document to just the multiset of its words, ignoring grammar and word order. (A _multiset_ is like a set, except that elements are allowed to appear more than once.)\n","\n","So, for example, the **bag of words** representation of \"I am Sam. Sam I am.\" (the first two lines of _Green Eggs and Ham_) would be `{I, I, am, am, Sam, Sam}`. In Python, it is easiest to represent multisets using dictionaries, where the keys are the (unique) words and the values are the counts. So we would represent the above bag of words as `{\"I\": 2, \"am\": 2, \"Sam\": 2}`.\n","\n","Let's convert the Dr. Seuss books to a bag of words representation. To do this, we will use the `Counter` object in the `collections` module of the Python standard library. First, let's see how the `Counter` works."]},{"cell_type":"code","metadata":{"id":"9oVrzGuEP9h3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651605843019,"user_tz":420,"elapsed":172,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"8b22e59d-e7fc-4bae-ec81-2f986a667506"},"source":["from collections import Counter\n","Counter([\"I\", \"am\", \"Sam\", \"Sam\", \"I\", \"am\"])"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'I': 2, 'Sam': 2, 'am': 2})"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"EsOSVRsbP9h7"},"source":["It takes in a list and returns a dictionary of counts---in other words, the bag of words representation that we want. But to be able to use `Counter`, we have to first convert our document into a list of words. We can do this using the string methods in Pandas, such as `.str.split()`, which splits a string into a list based on some character (which, by default, is whitespace)."]},{"cell_type":"code","metadata":{"id":"Qn0-18IhP9h8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651605867388,"user_tz":420,"elapsed":193,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"425f2167-dced-4cad-c78b-60bb9d3fa404"},"source":["docs_seuss.str.split()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                [I, am, Sam, I, am, Sam, Sam, I, am, That, Sam...\n","cat_in_the_hat                    [The, sun, did, not, shine., It, was, too, wet...\n","fox_in_socks                      [Fox, Socks, Box, Knox, Knox, in, box., Fox, i...\n","hop_on_pop                        [UP, PUP, Pup, is, up., CUP, PUP, Pup, in, cup...\n","horton_hears_a_who                [On, the, fifteenth, of, May,, in, the, jungle...\n","how_the_grinch_stole_christmas    [Every, Who, Down, in, Whoville, Liked, Christ...\n","oh_the_places_youll_go            [Congratulations!, Today, is, your, day., You'...\n","one_fish_two_fish                 [One, fish,, two, fish,, red, fish,, blue, fis...\n","dtype: object"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"86U_cqXUP9h_"},"source":["There are several problems with this approach:\n","\n","- **It is case-sensitive.**  The words \"PUP\" and \"Pup\" in _Hop on Pop_ are technically different strings and will be treated as different words by the `Counter`.\n","- **There is punctuation.**  For example, in _One Fish, Two Fish_, the words \"fish,\" and \"fish.\" will be treated as separate words.\n","\n","We can **normalize** the text for case by \n","\n","- converting all of the characters to lowercase, using the `.str.lower()` method\n","- stripping punctuation using a regular expression. The regular expression `[^\\w\\s]` tells Python to look for any pattern that is not (`^`) either an alphanumeric character (`\\w`) or whitespace (`\\s`). That is, it will detect any occurrence of punctuation. We will then use the `.str.replace()` method to replace all detected occurrences with whitespace, effectively removing all punctuation from the string.\n","\n","By chaining these commands together, we obtain a list, to which we can apply the `Counter` to obtain the bag of words representation."]},{"cell_type":"code","metadata":{"id":"Xp6l2xERP9iA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651607933055,"user_tz":420,"elapsed":142,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"697bd5dc-cca1-4cda-e965-1a0c746ba26a"},"source":["words = (\n","    docs_seuss.\n","    str.lower().\n","    str.replace(\"[^\\w\\s]\", \" \").\n","    str.split()\n",")\n","\n","words"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n","  after removing the cwd from sys.path.\n"]},{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                [i, am, sam, i, am, sam, sam, i, am, that, sam...\n","cat_in_the_hat                    [the, sun, did, not, shine, it, was, too, wet,...\n","fox_in_socks                      [fox, socks, box, knox, knox, in, box, fox, in...\n","hop_on_pop                        [up, pup, pup, is, up, cup, pup, pup, in, cup,...\n","horton_hears_a_who                [on, the, fifteenth, of, may, in, the, jungle,...\n","how_the_grinch_stole_christmas    [every, who, down, in, whoville, liked, christ...\n","oh_the_places_youll_go            [congratulations, today, is, your, day, you, r...\n","one_fish_two_fish                 [one, fish, two, fish, red, fish, blue, fish, ...\n","dtype: object"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"wEQBeIPsP9iD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651607934672,"user_tz":420,"elapsed":147,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"81ca34e1-3044-4e01-be73-c47cadbd1c47"},"source":["words.apply(Counter)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                {'i': 84, 'am': 16, 'sam': 19, 'that': 3, 'do'...\n","cat_in_the_hat                    {'the': 97, 'sun': 2, 'did': 10, 'not': 41, 's...\n","fox_in_socks                      {'fox': 17, 'socks': 19, 'box': 7, 'knox': 17,...\n","hop_on_pop                        {'up': 6, 'pup': 8, 'is': 12, 'cup': 4, 'in': ...\n","horton_hears_a_who                {'on': 21, 'the': 97, 'fifteenth': 1, 'of': 39...\n","how_the_grinch_stole_christmas    {'every': 5, 'who': 18, 'down': 10, 'in': 17, ...\n","oh_the_places_youll_go            {'congratulations': 1, 'today': 2, 'is': 7, 'y...\n","one_fish_two_fish                 {'one': 14, 'fish': 12, 'two': 4, 'red': 2, 'b...\n","dtype: object"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"LB1DVV7IP9iH"},"source":["## N-Grams\n","\n","The problem with the bag of words representation is that the ordering of the words is lost. For example, the following sentences have the exact same bag of words representation, but convey different meanings:\n","\n","1. The dog bit her owner.\n","2. Her dog bit the owner.\n","\n","The first sentence has only two actors (the dog and its owner), but the second sentence has three (a woman, her dog, and the owner of something). To better capture the _semantic_ meaning of these two documents, we can use **bigrams** instead of individual words. A **bigram** is simply a pair of consecutive words. The \"bag of bigrams\" of the two sentences above are quite different:\n","\n","1. {\"The dog\", \"dog bit\", \"bit her\", \"her owner\"}\n","2. {\"Her dog\", \"dog bit\", \"bit the\", \"the owner\"}\n","\n","They only share 1 bigram (out of 4) in common, despite sharing the same 5 words.\n","\n","Let's get the bag of bigrams representation for the words above. To generate the bigrams from the list of words, we will use the `zip` function in Python, which takes in two lists and returns a single list of pairs (consisting of one element from each list):"]},{"cell_type":"code","metadata":{"id":"P0sw5SnzP9iH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651605988057,"user_tz":420,"elapsed":162,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"a49f709e-67cf-4c7f-c2c6-19ed0e3ba57c"},"source":["list(zip([1, 2, 3], [4, 5, 6]))"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 4), (2, 5), (3, 6)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"UAJ_GxeJP9iK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651606001385,"user_tz":420,"elapsed":160,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"31126d12-05f5-4a19-f325-72e2aba17796"},"source":["def get_bigrams(words):\n","    # We need to line up the words as follows:\n","    #   words[0], words[1]\n","    #   words[1], words[2]\n","    #       ... ,  ...\n","    # words[n-1], words[n]\n","    #   words[n]\n","    # The first list is longer, so the last element in the first list is ignored.\n","    return zip(words, words[1:])\n","\n","words.apply(get_bigrams).apply(Counter)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                {('i', 'am'): 16, ('am', 'sam'): 2, ('sam', 'i...\n","cat_in_the_hat                    {('the', 'sun'): 2, ('sun', 'did'): 1, ('did',...\n","fox_in_socks                      {('fox', 'socks'): 1, ('socks', 'box'): 1, ('b...\n","hop_on_pop                        {('up', 'pup'): 1, ('pup', 'pup'): 2, ('pup', ...\n","horton_hears_a_who                {('on', 'the'): 5, ('the', 'fifteenth'): 1, ('...\n","how_the_grinch_stole_christmas    {('every', 'who'): 4, ('who', 'down'): 4, ('do...\n","oh_the_places_youll_go            {('congratulations', 'today'): 1, ('today', 'i...\n","one_fish_two_fish                 {('one', 'fish'): 1, ('fish', 'two'): 1, ('two...\n","dtype: object"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Xof8u3gHP9iO"},"source":["Instead of taking 2 words at a time, we could take 3, 4, or, in general, $n$ words. \n","A tuple of $n$ consecutive words is called an $n$-gram, and we can convert any document to a \"bag of $n$-grams\" representation. \n","\n","The larger $n$ is, the better the representation will capture the meaning of a document. But if $n$ is so large that hardly any $n$-gram occurs more than once, then we will not learn much from this representation."]},{"cell_type":"markdown","metadata":{"id":"C8itCO13P9iP"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"ah-BZi2BP9iQ"},"source":["1\\. Read in the OKCupid data set (`https://dlsun.github.io/pods/data/okcupid.csv`). Convert the users' responses to `essay0` (\"self summary\") into a bag of words representation. What word appears most often?\n","\n","(_Hint:_ Test your code on the first 100 users before testing it on the entire data set.)"]},{"cell_type":"code","source":["cupid = pd.read_csv(\"https://dlsun.github.io/pods/data/okcupid.csv\")\n","essay0 = cupid[\"essay0\"].dropna()"],"metadata":{"id":"-07OGB7-ZBto","executionInfo":{"status":"ok","timestamp":1651606484475,"user_tz":420,"elapsed":395,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["essays = (\n","    essay0.\n","    str.lower().\n","    str.replace(\"[^\\w\\s]\", \" \").\n","    str.split()\n",")\n","wordlist = essays.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HUFZwUvZgAM","executionInfo":{"status":"ok","timestamp":1651608672647,"user_tz":420,"elapsed":7383,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"5ec7d953-b834-4684-d9ee-27174138d5a9"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n","  after removing the cwd from sys.path.\n"]}]},{"cell_type":"code","source":["essay_dict = Counter(wordlist)\n","Counter.most_common(essay_dict)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBb3bgjhZ_Yt","executionInfo":{"status":"ok","timestamp":1651608679360,"user_tz":420,"elapsed":143,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"c2d42ace-bfa0-42bc-9728-6b70b0fabd9d"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('i', 22674)"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["**The word \"i\" appears most.**"],"metadata":{"id":"yLsa33wZf8zf"}},{"cell_type":"markdown","metadata":{"id":"Pj2oZSscP9iR"},"source":["2\\. Find the bag of trigrams representation for the Dr. Seuss books. How could you use the bag of trigrams to determine which book is the most linguistically diverse? Which book is it?"]},{"cell_type":"code","source":["def get_trigrams(words):\n","    return zip(words, words[1:], words[2:])\n","\n","bag_trigrams = words.apply(get_trigrams).apply(Counter)\n","bag_trigrams"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cR8zPGmFdeGX","executionInfo":{"status":"ok","timestamp":1651608597075,"user_tz":420,"elapsed":460,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"ab27f5ac-18b0-44e5-d342-8f2728d4913b"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                {('i', 'am', 'sam'): 2, ('am', 'sam', 'i'): 1,...\n","cat_in_the_hat                    {('the', 'sun', 'did'): 1, ('sun', 'did', 'not...\n","fox_in_socks                      {('fox', 'socks', 'box'): 1, ('socks', 'box', ...\n","hop_on_pop                        {('up', 'pup', 'pup'): 1, ('pup', 'pup', 'is')...\n","horton_hears_a_who                {('on', 'the', 'fifteenth'): 1, ('the', 'fifte...\n","how_the_grinch_stole_christmas    {('every', 'who', 'down'): 4, ('who', 'down', ...\n","oh_the_places_youll_go            {('congratulations', 'today', 'is'): 1, ('toda...\n","one_fish_two_fish                 {('one', 'fish', 'two'): 1, ('fish', 'two', 'f...\n","dtype: object"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["diverse = []\n","for book in bag_trigrams:\n","  diverse.append(sum(book.values()))\n","diverse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6F5KzzcZhVDf","executionInfo":{"status":"ok","timestamp":1651608812509,"user_tz":420,"elapsed":151,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"e419e723-14ce-460b-d1fc-4dc698615826"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[802, 1625, 892, 389, 2109, 1401, 1002, 1310]"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["max_val = max(diverse)\n","max_ind = diverse.index(max_val)\n","max_ind"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YvWHcs8jn41","executionInfo":{"status":"ok","timestamp":1651609030307,"user_tz":420,"elapsed":194,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"a2d6b9da-e782-468d-af1e-113082d1dc14"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["bag_trigrams.keys()[max_ind - 1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"f8hyxdcBle8E","executionInfo":{"status":"ok","timestamp":1651609502264,"user_tz":420,"elapsed":131,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"9148a934-f2e6-4f53-b809-80daf177f26b"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'hop_on_pop'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["**Hop on Pop is the most linguistically diverse Dr. Seuss book.**"],"metadata":{"id":"WdvLy8TXkfoi"}}]}