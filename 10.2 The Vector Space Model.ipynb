{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Copy of 10.2 The Vector Space Model.ipynb","provenance":[{"file_id":"https://github.com/dlsun/pods/blob/master/10-Textual-Data/10.2%20The%20Vector%20Space%20Model.ipynb","timestamp":1651605495320}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KYVDhxtNb74o"},"source":["# 10.2 The Vector Space Model\n","\n","In the previous section, we saw how a single document could be converted into a _bag of words_ (or, more precisely, a bag of $n$-grams) representation. In this section, we go one step further, converting an entire corpus of documents into tabular data."]},{"cell_type":"markdown","metadata":{"id":"-0WK4Y2qb74r"},"source":["## Term Frequencies\n","\n","The bag of words representation gives us a mapping between words and their counts, such as `{..., \"am\": 3, \"i\": 71, \"sam\": 6, ...}`. To turn the bag of words into a vector of numbers, we can simply take the word counts, as follows:\n","\n","| ... | i  | am | sam | ... |\n","|-----|----|----|-----|-----|\n","| ... | 71 |  3 |  6  | ... |\n","\n","If we do this for each document in the corpus, and stack the rows, we obtain a table of numbers called the _term-frequency matrix_. \n","\n","|        | ... | i  | am | sam | ... |\n","|--------|-----|----|----|-----|-----|\n","|**green_eggs_and_ham**| ... | 71 |  3 |  6  | ... |\n","|**cat_in_the_hat**| ... | 59 | 0 | 0 | ... |\n","|**fox_in_socks**| ... | 13 | 0 | 0 | ... |\n","|...|...|...|...|...|...|\n","|**one_fish_two_fish**| ... | 51 | 3 | 0 | ... |\n","\n","The columns are all words (or _terms_) that appear in the corpus, which collectively make up the _vocabulary_. The idea of representing documents by a vector of numbers is called the _vector space model_."]},{"cell_type":"markdown","metadata":{"id":"Cgl6jLw3cErP"},"source":["### Implementation from Scratch\n","\n","Let's obtain the term-frequency matrix for the Dr. Seuss books. First, we read in the data."]},{"cell_type":"code","metadata":{"id":"Q7Vy5COKb74s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610706505,"user_tz":420,"elapsed":1218,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"2b19ef65-84a7-4f59-939b-ea895c23947d"},"source":["import pandas as pd\n","import requests\n","\n","seuss_dir = \"http://dlsun.github.io/pods/data/drseuss/\"\n","seuss_files = [\n","    \"green_eggs_and_ham.txt\", \"cat_in_the_hat.txt\", \"fox_in_socks.txt\",\n","    \"hop_on_pop.txt\", \"horton_hears_a_who.txt\", \"how_the_grinch_stole_christmas.txt\",\n","    \"oh_the_places_youll_go.txt\", \"one_fish_two_fish.txt\"\n","]\n","\n","docs_seuss = pd.Series()\n","for file in seuss_files:\n","    response = requests.get(seuss_dir + file, \"r\")\n","    docs_seuss[file[:-4]] = response.text"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"markdown","metadata":{"id":"2UOASR79b74x"},"source":["Now we apply the bag of words representation to the normalized text."]},{"cell_type":"code","metadata":{"id":"A6VrqvvJb74y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610709964,"user_tz":420,"elapsed":190,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"55fc7c9f-4d28-4f17-8051-fb2daaf34e8e"},"source":["from collections import Counter\n","\n","bag_of_words = (\n","    docs_seuss.\n","    str.lower().                  # convert all letters to lowercase\n","    str.replace(\"[^\\w\\s]\", \" \").  # replace non-alphanumeric characters by whitespace\n","    str.split()                   # split on whitespace\n",").apply(Counter)\n","\n","bag_of_words"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n"]},{"output_type":"execute_result","data":{"text/plain":["green_eggs_and_ham                {'i': 84, 'am': 16, 'sam': 19, 'that': 3, 'do'...\n","cat_in_the_hat                    {'the': 97, 'sun': 2, 'did': 10, 'not': 41, 's...\n","fox_in_socks                      {'fox': 17, 'socks': 19, 'box': 7, 'knox': 17,...\n","hop_on_pop                        {'up': 6, 'pup': 8, 'is': 12, 'cup': 4, 'in': ...\n","horton_hears_a_who                {'on': 21, 'the': 97, 'fifteenth': 1, 'of': 39...\n","how_the_grinch_stole_christmas    {'every': 5, 'who': 18, 'down': 10, 'in': 17, ...\n","oh_the_places_youll_go            {'congratulations': 1, 'today': 2, 'is': 7, 'y...\n","one_fish_two_fish                 {'one': 14, 'fish': 12, 'two': 4, 'red': 2, 'b...\n","dtype: object"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"uvgU4114b742"},"source":["To turn this into a term-frequency matrix, we need to make a `DataFrame` out of it, where each column represents a word and each row a document---and each entry is the count of that word in the document."]},{"cell_type":"code","metadata":{"id":"ylxYncocb744","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1651610717328,"user_tz":420,"elapsed":478,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"1d825ffe-8920-4277-b671-1badcc9396f5"},"source":["tf = pd.DataFrame(list(bag_of_words))\n","tf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    i    am   sam  that    do  not  like  you  green  eggs  ...  zeds  upon  \\\n","0  84  16.0  19.0     3  36.0   82  44.0   34   10.0  10.0  ...   NaN   NaN   \n","1  59   NaN   NaN    25  25.0   41  14.0   34    NaN   NaN  ...   NaN   NaN   \n","2  13   NaN   NaN     6   8.0    1   1.0    8    NaN   NaN  ...   NaN   NaN   \n","3   2   1.0   NaN     5   NaN    2   6.0    2    NaN   NaN  ...   NaN   NaN   \n","4  43   1.0   NaN    36   7.0    7   NaN   47    NaN   NaN  ...   NaN   NaN   \n","5  16   NaN   NaN    16   4.0    2   2.0    2    NaN   NaN  ...   NaN   NaN   \n","6   6   NaN   NaN    12   4.0    9   1.0   85    NaN   NaN  ...   NaN   NaN   \n","7  51   3.0   NaN     1  12.0   10  21.0   24    NaN   NaN  ...   1.0   1.0   \n","\n","   heads  haircut  wave  swish  gack  park  clark  zeep  \n","0    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","1    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","2    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","3    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","4    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","5    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","6    NaN      NaN   NaN    NaN   NaN   NaN    NaN   NaN  \n","7    1.0      1.0   1.0    3.0   2.0   1.0    1.0   1.0  \n","\n","[8 rows x 1355 columns]"],"text/html":["\n","  <div id=\"df-93c1d4c4-b91c-4017-b4a1-c26c9a3c8894\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>i</th>\n","      <th>am</th>\n","      <th>sam</th>\n","      <th>that</th>\n","      <th>do</th>\n","      <th>not</th>\n","      <th>like</th>\n","      <th>you</th>\n","      <th>green</th>\n","      <th>eggs</th>\n","      <th>...</th>\n","      <th>zeds</th>\n","      <th>upon</th>\n","      <th>heads</th>\n","      <th>haircut</th>\n","      <th>wave</th>\n","      <th>swish</th>\n","      <th>gack</th>\n","      <th>park</th>\n","      <th>clark</th>\n","      <th>zeep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>84</td>\n","      <td>16.0</td>\n","      <td>19.0</td>\n","      <td>3</td>\n","      <td>36.0</td>\n","      <td>82</td>\n","      <td>44.0</td>\n","      <td>34</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>59</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>25</td>\n","      <td>25.0</td>\n","      <td>41</td>\n","      <td>14.0</td>\n","      <td>34</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>6</td>\n","      <td>8.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>6.0</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>43</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>36</td>\n","      <td>7.0</td>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>47</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>16</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>16</td>\n","      <td>4.0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>12</td>\n","      <td>4.0</td>\n","      <td>9</td>\n","      <td>1.0</td>\n","      <td>85</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>51</td>\n","      <td>3.0</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>12.0</td>\n","      <td>10</td>\n","      <td>21.0</td>\n","      <td>24</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 1355 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93c1d4c4-b91c-4017-b4a1-c26c9a3c8894')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-93c1d4c4-b91c-4017-b4a1-c26c9a3c8894 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-93c1d4c4-b91c-4017-b4a1-c26c9a3c8894');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"lAYV8ZPtb747"},"source":["This matrix is full of missing numbers. A missing number means that the word did not appear in that document. In other words, a count of `NaN` really means a count of 0. So it makes sense in this situation to replace the `NaN`s by 0s."]},{"cell_type":"code","metadata":{"id":"4p9mciwPb748","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1651610722568,"user_tz":420,"elapsed":188,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"8915e55f-4ea3-44c2-a350-dd56603ef91f"},"source":["tf = tf.fillna(0)\n","tf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    i    am   sam  that    do  not  like  you  green  eggs  ...  zeds  upon  \\\n","0  84  16.0  19.0     3  36.0   82  44.0   34   10.0  10.0  ...   0.0   0.0   \n","1  59   0.0   0.0    25  25.0   41  14.0   34    0.0   0.0  ...   0.0   0.0   \n","2  13   0.0   0.0     6   8.0    1   1.0    8    0.0   0.0  ...   0.0   0.0   \n","3   2   1.0   0.0     5   0.0    2   6.0    2    0.0   0.0  ...   0.0   0.0   \n","4  43   1.0   0.0    36   7.0    7   0.0   47    0.0   0.0  ...   0.0   0.0   \n","5  16   0.0   0.0    16   4.0    2   2.0    2    0.0   0.0  ...   0.0   0.0   \n","6   6   0.0   0.0    12   4.0    9   1.0   85    0.0   0.0  ...   0.0   0.0   \n","7  51   3.0   0.0     1  12.0   10  21.0   24    0.0   0.0  ...   1.0   1.0   \n","\n","   heads  haircut  wave  swish  gack  park  clark  zeep  \n","0    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","1    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","2    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","3    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","4    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","5    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","6    0.0      0.0   0.0    0.0   0.0   0.0    0.0   0.0  \n","7    1.0      1.0   1.0    3.0   2.0   1.0    1.0   1.0  \n","\n","[8 rows x 1355 columns]"],"text/html":["\n","  <div id=\"df-40ad1ece-4586-4d3c-be37-3e476afc212a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>i</th>\n","      <th>am</th>\n","      <th>sam</th>\n","      <th>that</th>\n","      <th>do</th>\n","      <th>not</th>\n","      <th>like</th>\n","      <th>you</th>\n","      <th>green</th>\n","      <th>eggs</th>\n","      <th>...</th>\n","      <th>zeds</th>\n","      <th>upon</th>\n","      <th>heads</th>\n","      <th>haircut</th>\n","      <th>wave</th>\n","      <th>swish</th>\n","      <th>gack</th>\n","      <th>park</th>\n","      <th>clark</th>\n","      <th>zeep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>84</td>\n","      <td>16.0</td>\n","      <td>19.0</td>\n","      <td>3</td>\n","      <td>36.0</td>\n","      <td>82</td>\n","      <td>44.0</td>\n","      <td>34</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>59</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>25</td>\n","      <td>25.0</td>\n","      <td>41</td>\n","      <td>14.0</td>\n","      <td>34</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6</td>\n","      <td>8.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>8</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>6.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>43</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>36</td>\n","      <td>7.0</td>\n","      <td>7</td>\n","      <td>0.0</td>\n","      <td>47</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>16</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>16</td>\n","      <td>4.0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12</td>\n","      <td>4.0</td>\n","      <td>9</td>\n","      <td>1.0</td>\n","      <td>85</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>51</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>12.0</td>\n","      <td>10</td>\n","      <td>21.0</td>\n","      <td>24</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 1355 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40ad1ece-4586-4d3c-be37-3e476afc212a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-40ad1ece-4586-4d3c-be37-3e476afc212a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-40ad1ece-4586-4d3c-be37-3e476afc212a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"YbTiSkqNb75B"},"source":["### Implementation using `scikit-learn`\n","\n","We could have also used the `CountVectorizer` in `scikit-learn` to obtain the term-frequency matrix. This vectorizer is fit to a list of the documents in the corpus. By default, it converts all letters to lowercase and strips punctuation, although this behavior can be customized using the `strip_accents=` and `lowercase=` parameters. "]},{"cell_type":"code","metadata":{"id":"Fhl2Kwb5b75C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610783719,"user_tz":420,"elapsed":619,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"32b927db-5e52-471d-db25-67b203243283"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vec = CountVectorizer()\n","vec.fit(docs_seuss) # This determines the vocabulary.\n","tf_sparse = vec.transform(docs_seuss)\n","\n","tf_sparse"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<8x1344 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 2308 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"4ni61EAub75G"},"source":["Notice that `CountVectorizer` returns the term-frequency matrix, not as a `DataFrame` or even as a `numpy` array, but as a `scipy` sparse matrix. A _sparse matrix_ is one whose entries are mostly zeroes. For example,\n","\n","$$ \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 1.7 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -0.8 & 0 \\end{pmatrix} $$\n","\n","is an example of a sparse matrix. Instead of storing 20 values (most of which are equal to 0), we can simply store the locations of the non-zero entries and their values:\n","\n","- $(1, 0) \\rightarrow 1.7$\n","- $(3, 3) \\rightarrow -0.8$\n","\n","All other entries of the matrix are assumed to be zero. This representation offers substantial memory savings when there are only a few non-zero entries. (But if not, then this representation can actually be more expensive.) Term-frequency matrices are usually sparse because most words do not appear in all documents.\n","\n","The `scipy` sparse matrix format is used to store sparse matrices. If necessary, a `scipy` sparse matrix can be converted to a `numpy` matrix using the `.todense()` method."]},{"cell_type":"code","metadata":{"id":"61q9l5bbb75H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610874484,"user_tz":420,"elapsed":178,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"d9035322-8926-4c66-ce93-9869b27b9f0a"},"source":["tf_sparse.todense()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["matrix([[0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 1, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 3, 1, 1]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"ibRYltDdb75L"},"source":["We can further convert this `numpy` matrix to a `pandas` `DataFrame`. To make the column names descriptive, we call the `.get_feature_names()` method of the `CountVectorizer`, which returns a list of the words in the order that they appear in the matrix."]},{"cell_type":"code","metadata":{"id":"NKAtlcEDb75M","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"ok","timestamp":1651610883449,"user_tz":420,"elapsed":215,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"0d1da52e-78ee-463c-8de9-38a2105fd54f"},"source":["pd.DataFrame(\n","    tf_sparse.todense(),\n","    columns=vec.get_feature_names()\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["   12  56  98  able  about  act  afraid  after  afternoon  again  ...  yop  \\\n","0   0   0   0     0      0    0       0      0          0      0  ...    0   \n","1   0   0   0     0      3    0       0      1          0      0  ...    0   \n","2   0   0   0     0      2    0       0      0          0      0  ...    0   \n","3   0   0   0     0      0    0       0      2          0      0  ...    0   \n","4   1   1   0     1      1    0       0      4          2      1  ...    0   \n","5   0   0   0     0      0    0       0      0          0      0  ...    0   \n","6   0   0   1     0      1    1       2      0          0      0  ...    0   \n","7   0   0   0     0      1    0       0      0          0      2  ...    1   \n","\n","   yopp  you  young  your  yourself  yourselves  zans  zeds  zeep  \n","0     0   34      0     0         0           0     0     0     0  \n","1     0   34      0     8         0           0     0     0     0  \n","2     0    8      0     1         0           0     0     0     0  \n","3     0    2      0     0         0           0     0     0     0  \n","4     3   47      5     7         0           1     0     0     0  \n","5     0    2      1     0         0           0     0     0     0  \n","6     0   85      0    20         2           0     0     0     0  \n","7     0   24      0     9         0           0     3     1     1  \n","\n","[8 rows x 1344 columns]"],"text/html":["\n","  <div id=\"df-13847212-8818-4c59-95ae-3e445a981da1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>12</th>\n","      <th>56</th>\n","      <th>98</th>\n","      <th>able</th>\n","      <th>about</th>\n","      <th>act</th>\n","      <th>afraid</th>\n","      <th>after</th>\n","      <th>afternoon</th>\n","      <th>again</th>\n","      <th>...</th>\n","      <th>yop</th>\n","      <th>yopp</th>\n","      <th>you</th>\n","      <th>young</th>\n","      <th>your</th>\n","      <th>yourself</th>\n","      <th>yourselves</th>\n","      <th>zans</th>\n","      <th>zeds</th>\n","      <th>zeep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>47</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 1344 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13847212-8818-4c59-95ae-3e445a981da1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-13847212-8818-4c59-95ae-3e445a981da1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-13847212-8818-4c59-95ae-3e445a981da1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"YnompYDTb75P"},"source":["The term-frequency matrix that `CountVectorizer` produced is not exactly the same as the matrix that we produced ourselves using just `pandas`. Although the two matrices have the same number of rows (8, corresponding to the number of documents in the corpus), they have a different number of columns. It appears that `CountVectorizer` had a vocabulary that was 11 words smaller (1344 words instead of 1355). We can determine exactly which 11 words these are, by taking the set difference:"]},{"cell_type":"code","metadata":{"id":"IK31TXDvb75Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610907913,"user_tz":420,"elapsed":176,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"d8c31a8e-e8fa-4f7b-8389-b0d7af45cc1a"},"source":["set(tf.columns) - set(vec.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["{'3', '4', '6', 'a', 'd', 'i', 'j', 'm', 'o', 's', 't'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"0tF1gsPqb75U"},"source":["We see that all of the words that `CountVectorizer` missed were one-character long. By default, `CountVectorizer` only retains words that are at least 2 characters long. This behavior can be customized using the `token_pattern=` parameter, but we will not pursue that here, since 1-letter words are usually not useful for analysis anyway."]},{"cell_type":"markdown","metadata":{"id":"X5Pt4YzPb75V"},"source":["`CountVectorizer` can even count $n$-grams. If we wanted both unigrams (i.e., individual words) and bigrams, then we would specify `ngram_range=(1, 2)`. If we wanted only the bigrams, then we would specify `ngram_range=(2, 2)`. \n","\n","Let's get unigrams, bigrams, and trigrams."]},{"cell_type":"code","metadata":{"id":"xXPmH0rPb75W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610957787,"user_tz":420,"elapsed":394,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"2eb2e0c8-6a17-4ce8-8d78-c8962b712ca5"},"source":["vec = CountVectorizer(ngram_range=(1, 3))\n","vec.fit(docs_seuss)\n","vec.transform(docs_seuss)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<8x14918 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 16560 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"rRWacCYmb75Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651610966770,"user_tz":420,"elapsed":185,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"e13df455-8c40-4c27-b19c-facbefde62d0"},"source":["# number of non-zero values in the sparse matrix.\n","vec.transform(docs_seuss).count_nonzero()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16560"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"qbOmJw3Hb75c"},"source":["There are nearly 15,000 bigrams. If we wanted to store this data in a `DataFrame`, we would need as many columns, even though only about 16,000 out of the nearly 120,000 entries are nonzero. This is why sparse matrices are vital in text processing."]},{"cell_type":"markdown","metadata":{"id":"PvqfLNhNb75d"},"source":["## TF-IDF\n","\n","The problem with term frequencies (TF) is that common words like \"the\" and \"that\" tend to have high counts and dominate. A better indicator of whether two documents are similar is if they share rare words. For example, the word \"eat\" only appears in two of the Dr. Seuss stories. The presence of that word in two documents is a strong indicator that the documents are similar, so we should give more weight to terms like it.\n","\n","This is the idea behind TF-IDF. We take each term frequency and re-weight it according to how many documents that term appears in (i.e., the **document frequency**). Since we want words that appear in fewer documents to get more weight, we take the **inverse document frequency** (IDF).  We take the logarithm of IDF because the distribution of IDFs is heavily skewed to the right. (Remember the discussion about transforming data from Chapter 1.4.) So in the end, the formula for IDF is:\n","\n","$$ \\textrm{idf}(t, D) = \\log \\frac{\\text{# of documents}}{\\text{# of documents containing $t$}} = \\log \\frac{|D|}{|d \\in D: t \\in d|}. $$\n","\n","(Sometimes, $1$ will be added to the denominator to prevent division by zero, if there are terms in the vocabulary that do not appear in the corpus.)\n","\n","To calculate TF-IDF, we simply multiply the term frequencies by the inverse document frequencies:\n","\n","$$ \\textrm{tf-idf}(d, t, D) = \\textrm{tf}(d, t) \\cdot \\textrm{idf}(t, D). $$\n","\n","Notice that unlike TF, the TF-IDF representation of a given document depends on the entire corpus of documents."]},{"cell_type":"markdown","metadata":{"id":"j7TZxq1jcP9A"},"source":["### Implementation from Scratch\n","\n","Let's first see how to calculate TF-IDF from scratch, using the term-frequency matrix we obtained above."]},{"cell_type":"code","metadata":{"id":"z6-b9Lgdb75e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611019500,"user_tz":420,"elapsed":190,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"009e8707-d972-4f2e-e3b6-ca0853b352ea"},"source":["# Get document frequencies \n","# (How many documents does each word appear in?)\n","df = (tf > 0).sum(axis=0)\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["i        8\n","am       4\n","sam      1\n","that     8\n","do       7\n","        ..\n","swish    1\n","gack     1\n","park     1\n","clark    1\n","zeep     1\n","Length: 1355, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"NP10ysjOb75i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611024134,"user_tz":420,"elapsed":232,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"44a56141-1e89-4ae0-de46-2e7909c55715"},"source":["import numpy as np\n","\n","# Get IDFs\n","idf = np.log(len(tf) / df)\n","idf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["i        0.000000\n","am       0.693147\n","sam      2.079442\n","that     0.000000\n","do       0.133531\n","           ...   \n","swish    2.079442\n","gack     2.079442\n","park     2.079442\n","clark    2.079442\n","zeep     2.079442\n","Length: 1355, dtype: float64"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"H38ZejUGb75m","colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"status":"ok","timestamp":1651611027246,"user_tz":420,"elapsed":150,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"a017f5d7-90e4-4dd1-b042-e62d96d26828"},"source":["# Calculate TF-IDFs\n","tf_idf = tf * idf\n","tf_idf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     i         am        sam  that        do  not      like  you      green  \\\n","0  0.0  11.090355  39.509389   0.0  4.807130  0.0  5.875381  0.0  20.794415   \n","1  0.0   0.000000   0.000000   0.0  3.338285  0.0  1.869439  0.0   0.000000   \n","2  0.0   0.000000   0.000000   0.0  1.068251  0.0  0.133531  0.0   0.000000   \n","3  0.0   0.693147   0.000000   0.0  0.000000  0.0  0.801188  0.0   0.000000   \n","4  0.0   0.693147   0.000000   0.0  0.934720  0.0  0.000000  0.0   0.000000   \n","5  0.0   0.000000   0.000000   0.0  0.534126  0.0  0.267063  0.0   0.000000   \n","6  0.0   0.000000   0.000000   0.0  0.534126  0.0  0.133531  0.0   0.000000   \n","7  0.0   2.079442   0.000000   0.0  1.602377  0.0  2.804159  0.0   0.000000   \n","\n","        eggs  ...      zeds      upon     heads   haircut      wave     swish  \\\n","0  20.794415  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","1   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","2   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","3   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","4   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","5   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","6   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n","7   0.000000  ...  2.079442  2.079442  2.079442  2.079442  2.079442  6.238325   \n","\n","       gack      park     clark      zeep  \n","0  0.000000  0.000000  0.000000  0.000000  \n","1  0.000000  0.000000  0.000000  0.000000  \n","2  0.000000  0.000000  0.000000  0.000000  \n","3  0.000000  0.000000  0.000000  0.000000  \n","4  0.000000  0.000000  0.000000  0.000000  \n","5  0.000000  0.000000  0.000000  0.000000  \n","6  0.000000  0.000000  0.000000  0.000000  \n","7  4.158883  2.079442  2.079442  2.079442  \n","\n","[8 rows x 1355 columns]"],"text/html":["\n","  <div id=\"df-0f2616e9-9fb0-401a-a126-90ee221c5af8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>i</th>\n","      <th>am</th>\n","      <th>sam</th>\n","      <th>that</th>\n","      <th>do</th>\n","      <th>not</th>\n","      <th>like</th>\n","      <th>you</th>\n","      <th>green</th>\n","      <th>eggs</th>\n","      <th>...</th>\n","      <th>zeds</th>\n","      <th>upon</th>\n","      <th>heads</th>\n","      <th>haircut</th>\n","      <th>wave</th>\n","      <th>swish</th>\n","      <th>gack</th>\n","      <th>park</th>\n","      <th>clark</th>\n","      <th>zeep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>11.090355</td>\n","      <td>39.509389</td>\n","      <td>0.0</td>\n","      <td>4.807130</td>\n","      <td>0.0</td>\n","      <td>5.875381</td>\n","      <td>0.0</td>\n","      <td>20.794415</td>\n","      <td>20.794415</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>3.338285</td>\n","      <td>0.0</td>\n","      <td>1.869439</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.068251</td>\n","      <td>0.0</td>\n","      <td>0.133531</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.693147</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.801188</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.693147</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.934720</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.534126</td>\n","      <td>0.0</td>\n","      <td>0.267063</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.534126</td>\n","      <td>0.0</td>\n","      <td>0.133531</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.0</td>\n","      <td>2.079442</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.602377</td>\n","      <td>0.0</td>\n","      <td>2.804159</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>2.079442</td>\n","      <td>2.079442</td>\n","      <td>2.079442</td>\n","      <td>2.079442</td>\n","      <td>2.079442</td>\n","      <td>6.238325</td>\n","      <td>4.158883</td>\n","      <td>2.079442</td>\n","      <td>2.079442</td>\n","      <td>2.079442</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 1355 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f2616e9-9fb0-401a-a126-90ee221c5af8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0f2616e9-9fb0-401a-a126-90ee221c5af8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0f2616e9-9fb0-401a-a126-90ee221c5af8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"rIhNpIHfb75q"},"source":["### Implementation using `scikit-learn`\n","\n","We will not generally implement TF-IDF from scratch, like we did above. Instead, we will use Scikit-Learn's `TfidfVectorizer`, which operates similarly to `CountVectorizer`, except that it returns a matrix of the TF-IDF weights."]},{"cell_type":"code","metadata":{"id":"Fbu5oi7ib75r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611036164,"user_tz":420,"elapsed":181,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"7e64a5a0-278a-4830-dceb-f2e9c81019a2"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vec = TfidfVectorizer(norm=None) # Do not normalize.\n","vec.fit(docs_seuss) # This determines the vocabulary.\n","tf_idf_sparse = vec.transform(docs_seuss)\n","tf_idf_sparse"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<8x1344 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 2308 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"cycHn7Cjb75v"},"source":["## Cosine Similarity\n","\n","We now have a representation of each text document as a vector of numbers. Each number can either be a term frequency or a TF-IDF weight. We can visualize each vector as an arrow in a high-dimensional space, where each dimension represents a word. The magnitude of the vector along a dimension represents the \"frequency\" (TF or TF-IDF) of that word in the document. For example, if our vocabulary only contains two words, \"i\" and \"sam\", then the arrows shown below might represent two documents:\n","\n","<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space.png?raw=1\" width=\"300\"/>\n","\n","To fit $k$-nearest neighbors or $k$-means clustering, we need some way to measure the distance between two documents (i.e., two vectors). We could use Euclidean distance, as we have done in the past.\n","\n","<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_euclidean.png?raw=1\" width=\"300\"/>\n","\n","But Euclidean distance does not make sense for TF or TF-IDF vectors. To see why, consider the two documents:\n","\n","1. \"I am Sam.\" \n","2. \"I am Sam. Sam I am.\" \n","\n","The two documents are obviously very similar. But the vector for the second is twice as long as the vector for the first because the second document has twice as many occurrences of each word. This is true whether we use TF or TF-IDF weights. If we calculate the Euclidean distance between these two vectors, then they will seem quite far apart.\n","\n","<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_example.png?raw=1\" width=\"300\"/>\n","\n","With TF and TF-IDF vectors, the distinguishing property is their _direction_. Because the two vectors above point in the same direction, they are similar. We need a distance metric that measures how different their directions are. A natural way to measure the difference between the directions of two vectors is the angle between them.\n","\n","<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_cosine.png?raw=1\" width=\"300\"/>\n","\n","The cosine of the angle between two vectors ${\\bf a}$ and ${\\bf b}$ can be calculated as:\n","\n","$$ \\cos \\theta = \\frac{\\sum a_j b_j}{\\sqrt{\\sum a_j^2} \\sqrt{\\sum b_j^2}}. $$\n","\n","Although it is possible to work out the angle $\\theta$ from this formula, it is more common to report $\\cos\\theta$ as a measure of similarity between two vectors. This similarity metric is called **cosine similarity**. Notice that when the angle $\\theta$ is close to 0 (i.e., when the two vectors point in nearly the same direction), the value of $\\cos\\theta$ is high (close to 1.0, which is the maximum possible value).\n","\n","The cosine _distance_ is defined as 1 minus the similarity. This makes it so that 0 means that the two vectors point in the same direction:\n","\n","$$ d_{\\cos}({\\bf a}, {\\bf b}) = 1 - \\cos(\\theta({\\bf a}, {\\bf b})) = 1 - \\frac{\\sum a_j b_j}{\\sqrt{\\sum a_j^2} \\sqrt{\\sum b_j^2}}. $$"]},{"cell_type":"markdown","metadata":{"id":"5rQbvOkVcbvK"},"source":["### Implementation from Scratch\n","\n","Let's calculate the cosine similarity between document 0 (_Green Eggs and Ham_) and document 2 (_Fox in Socks_) using the TF-IDF representation."]},{"cell_type":"code","metadata":{"id":"_kagfHiKb75w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611114548,"user_tz":420,"elapsed":168,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"ad836b09-a3d7-47b5-9e29-d4d7fa275ea4"},"source":["# Calculate the numerator.\n","a = tf_idf_sparse[0, :]\n","b = tf_idf_sparse[2, :]\n","dot = a.multiply(b).sum()\n","\n","# Calculate the terms in the denominator.\n","a_len = np.sqrt(a.multiply(a).sum())\n","b_len = np.sqrt(b.multiply(b).sum())\n","\n","# Cosine similarity is their ratio.\n","dot / (a_len * b_len)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.10197809112431884"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"Bqj82-h2b750"},"source":["These two vectors are not very similar, as evidenced by their low cosine similarity (close to 0.0). Let's try to find the most similar documents in the corpus to _Green Eggs and Ham_---in other words, its nearest neighbors. To do this, we will take advantage of _broadcasting_: we will multiply a TF-IDF vector (representing document 0) by the entire TF-IDF matrix and calculate the sum over the columns. This will give us a vector of dot products."]},{"cell_type":"code","metadata":{"id":"7Nr27ArFb751","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611122217,"user_tz":420,"elapsed":192,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"f8c44536-50a0-4805-f0b0-d9bc33a21b91"},"source":["# Calculate the numerators.\n","a = tf_idf_sparse[0, :]\n","B = tf_idf_sparse\n","dot = a.multiply(B).sum(axis=1)\n","dot"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["matrix([[34282.66548742],\n","        [13856.74242973],\n","        [ 3192.73842529],\n","        [ 1662.65737991],\n","        [ 8698.41557824],\n","        [ 5098.5714281 ],\n","        [ 6918.05569539],\n","        [ 6958.3624518 ]])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"_tuXVdFAb754","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611126993,"user_tz":420,"elapsed":214,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"9c46c8c9-3af0-4e49-ea8f-be1e7ea52b43"},"source":["# Calculate the denominators.\n","a_len = np.sqrt(a.multiply(a).sum())\n","b_len = np.sqrt(B.multiply(B).sum(axis=1))\n","print(a_len)\n","b_len"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["185.15578707514322\n"]},{"output_type":"execute_result","data":{"text/plain":["matrix([[185.15578708],\n","        [220.67601821],\n","        [169.09048515],\n","        [ 67.77450402],\n","        [246.13363988],\n","        [208.51785767],\n","        [151.24831788],\n","        [151.8499661 ]])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"m72blwPNb76A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611129101,"user_tz":420,"elapsed":155,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"7f267af6-fb18-4599-eb60-c3151870f9c6"},"source":["# Calculate their ratio to obtain cosine similarities.\n","dot / (a_len * b_len)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["matrix([[1.        ],\n","        [0.33913196],\n","        [0.10197809],\n","        [0.13249489],\n","        [0.19086746],\n","        [0.13205899],\n","        [0.2470337 ],\n","        [0.24748852]])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"ZEuizyNvb76E"},"source":["Now let's put this matrix into a `DataFrame` so that we can easily sort the values in descending order."]},{"cell_type":"code","metadata":{"id":"1IWk1cYEb76E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611131134,"user_tz":420,"elapsed":162,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"a7223187-0726-4b9e-d266-90c350c638ad"},"source":["cos_similarities = pd.DataFrame(dot / (a_len * b_len))[0]\n","most_similar = cos_similarities.sort_values(ascending=False)\n","most_similar"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    1.000000\n","1    0.339132\n","7    0.247489\n","6    0.247034\n","4    0.190867\n","3    0.132495\n","5    0.132059\n","2    0.101978\n","Name: 0, dtype: float64"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"ELG94ygFb76H"},"source":["Of course, the most similar document in the corpus to _Green Eggs and Ham_ (with a perfect cosine similarity of 1.0) is itself. But the next most similar text is _The Cat in the Hat_."]},{"cell_type":"markdown","metadata":{"id":"eBVAM4hmb76I"},"source":["### Implementation using scikit-learn\n","\n","It is also possible to calculate cosine similarities/distances in `scikit-learn` using the same API that we used to calculate distances in Chapter 3."]},{"cell_type":"code","metadata":{"id":"0ZoMACMMb76J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611161511,"user_tz":420,"elapsed":170,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"a15b11cf-69cb-46bb-9ce2-082c94045afd"},"source":["from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n","\n","cosine_similarity(tf_idf_sparse)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.        , 0.33913196, 0.10197809, 0.13249489, 0.19086746,\n","        0.13205899, 0.2470337 , 0.24748852],\n","       [0.33913196, 1.        , 0.19001503, 0.33209661, 0.58896907,\n","        0.53943258, 0.44251739, 0.61299975],\n","       [0.10197809, 0.19001503, 1.        , 0.0970933 , 0.17120801,\n","        0.12888222, 0.15449203, 0.19315925],\n","       [0.13249489, 0.33209661, 0.0970933 , 1.        , 0.25413388,\n","        0.2141914 , 0.14717619, 0.36871962],\n","       [0.19086746, 0.58896907, 0.17120801, 0.25413388, 1.        ,\n","        0.59455601, 0.48514399, 0.48084797],\n","       [0.13205899, 0.53943258, 0.12888222, 0.2141914 , 0.59455601,\n","        1.        , 0.3009526 , 0.37778308],\n","       [0.2470337 , 0.44251739, 0.15449203, 0.14717619, 0.48514399,\n","        0.3009526 , 1.        , 0.40455217],\n","       [0.24748852, 0.61299975, 0.19315925, 0.36871962, 0.48084797,\n","        0.37778308, 0.40455217, 1.        ]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"mTWQae5xb76M"},"source":["The $(i, j)$th entry of this matrix represents the cosine similarity between the $i$th and $j$th documents. So the first row of this matrix contains the similarities between _Green Eggs and Ham_ and the other documents in the corpus. Check that these numbers match the ones we obtained manually."]},{"cell_type":"code","metadata":{"id":"vm7f6Jncb76N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651611176997,"user_tz":420,"elapsed":236,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"2e7d0d4c-a371-4709-bbbe-96428eafc10f"},"source":["cosine_similarity(tf_idf_sparse)[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.        , 0.33913196, 0.10197809, 0.13249489, 0.19086746,\n","       0.13205899, 0.2470337 , 0.24748852])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"GMu61Qx5b76Q"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"vBojZWgHb76R"},"source":["1\\. Suppose we had instead compared documents using cosine similarity on the term frequencies (TF), instead of TF-IDF. Does this change the conclusion?"]},{"cell_type":"code","source":["cosine_similarity(tf_sparse)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6qBcB9wsvZt","executionInfo":{"status":"ok","timestamp":1651611488590,"user_tz":420,"elapsed":9,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"07626742-fd23-4cc1-948a-8ac7faddda5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.        , 0.45635478, 0.21942681, 0.20985253, 0.28631398,\n","       0.1962546 , 0.35377095, 0.34495765])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["**The results are slightly different, but the conclusion remains the same.**"],"metadata":{"id":"g9_eNnOrtUV1"}},{"cell_type":"markdown","metadata":{"id":"uEupF68-b76R"},"source":["2\\. Suppose we had instead used Euclidean distance on the TF-IDF weights, instead of cosine distance. Does this change the conclusion? What if we first normalize the length of the TF-IDF vector for each document before calculating Euclidean distance?\n","\n","_Challenge Exercise:_ Can you prove the above fact mathematically?"]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import euclidean_distances\n","\n","euclidean_distances(tf_idf_sparse)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16prVOQWtcny","executionInfo":{"status":"ok","timestamp":1651611493222,"user_tz":420,"elapsed":4,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"befe6a32-8d5f-4993-a3c1-c9999de7da7a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  0.        , 235.08952687, 237.67368555, 188.54902313,\n","       278.33002535, 259.933106  , 208.14083636, 208.38510692])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["from sklearn.preprocessing import Normalizer\n","\n","scaler = Normalizer()\n","scaler.fit(tf_idf_sparse)\n","tf_idf_sparse_norm = scaler.transform(tf_idf_sparse)\n","\n","tf_idf_sparse_norm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9N7aZSwduc-r","executionInfo":{"status":"ok","timestamp":1651611763004,"user_tz":420,"elapsed":169,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"da24b7c1-107d-42ed-d37d-ce9bd63d2224"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<8x1344 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 2308 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["euclidean_distances(tf_idf_sparse_norm)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1D5AdbyJu4qw","executionInfo":{"status":"ok","timestamp":1651611780050,"user_tz":420,"elapsed":187,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"c1c297ed-379a-471c-ba09-ccef0cf4c0f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.        , 1.14966782, 1.34016559, 1.31719787, 1.27211048,\n","       1.31752876, 1.22716445, 1.22679377])"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["**This changes the conclusion. Normalizing leads to a different conclusion.**"],"metadata":{"id":"kLQhC4Ovt3fr"}},{"cell_type":"markdown","metadata":{"id":"j_jiS5xzpWx2"},"source":["3\\. Convert the self-summary variable (`essay0`) in the OKCupid data set (https://dlsun.github.io/pods/data/okcupid.csv) to a TF-IDF representation. Use this to find a match for user 61 based on what he says he is looking for in a partner (`essay9`).\n","\n","The [data dictionary](https://github.com/rudeboybert/JSE_OkCupid/blob/master/okcupid_codebook.txt) may help you understand what the columns mean."]},{"cell_type":"code","source":["cupid = pd.read_csv(\"https://dlsun.github.io/pods/data/okcupid.csv\")\n","essay0 = cupid[\"essay0\"].fillna(\"\")"],"metadata":{"id":"ok3_hodGvFor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vec = TfidfVectorizer(norm=None)\n","vec.fit(essay0)\n","tf_idf_essay0 = vec.transform(essay0)\n","tf_idf_essay0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lnXSXQpjvkDR","executionInfo":{"status":"ok","timestamp":1651614468919,"user_tz":420,"elapsed":1150,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"da8ff7bb-3d16-460a-927f-b52a834278ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<3000x16562 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 206525 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["tf_idf_essay9 = vec.transform(cupid.loc[[61]][\"essay9\"])"],"metadata":{"id":"aQwyKcvkwmNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sim = cosine_similarity(tf_idf_essay0, tf_idf_essay9)"],"metadata":{"id":"5G0Yo5fZ6RF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sim"],"metadata":{"id":"N5jY_skZ6Xx-","executionInfo":{"status":"ok","timestamp":1651614780224,"user_tz":420,"elapsed":167,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"e0a199a2-eef5-4fc3-c098-0267c41f926b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.00887211],\n","       [0.03697228],\n","       [0.        ],\n","       ...,\n","       [0.        ],\n","       [0.02616639],\n","       [0.09104736]])"]},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"9JUuovrSb76S"},"source":["Exercises 4-5 ask you to work with the Enron spam data set (https://dlsun.github.io/pods/data/enron_spam.csv). This data set contains the subjects and bodies of a sample of e-mails that the Federal Energy Regulatory Commission (FERC) collected during their 2002 investigation of the energy company Enron. "]},{"cell_type":"code","source":["enron = pd.read_csv(\"https://dlsun.github.io/pods/data/enron_spam.csv\")\n","enron.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"sN5SPRYZxF4O","executionInfo":{"status":"ok","timestamp":1651614017415,"user_tz":420,"elapsed":382,"user":{"displayName":"Sophia Chung","userId":"18074224600171182532"}},"outputId":"e8b950ad-eb5f-430d-c156-c6cb4db54acd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                              subject  \\\n","0                       mirant 4 / 01   \n","1                    re : lobo payout   \n","2                 entex transaction 7   \n","3        re : hpl transport contracts   \n","4  welcome to aol instant messenger !   \n","\n","                                                body  spam  \n","0  we invoiced mirant americas for deal 705989 an...     0  \n","1  because the payback was done for october 2001 ...     0  \n","2  for december 1999 , since the volumes for tran...     0  \n","3  i would think that the first contract should g...     0  \n","4  welcome to the aol instant messenger ( sm ) se...     0  "],"text/html":["\n","  <div id=\"df-c68346c9-4d39-4898-a270-76d5f1f12daa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>subject</th>\n","      <th>body</th>\n","      <th>spam</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>mirant 4 / 01</td>\n","      <td>we invoiced mirant americas for deal 705989 an...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>re : lobo payout</td>\n","      <td>because the payback was done for october 2001 ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>entex transaction 7</td>\n","      <td>for december 1999 , since the volumes for tran...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>re : hpl transport contracts</td>\n","      <td>i would think that the first contract should g...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>welcome to aol instant messenger !</td>\n","      <td>welcome to the aol instant messenger ( sm ) se...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c68346c9-4d39-4898-a270-76d5f1f12daa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c68346c9-4d39-4898-a270-76d5f1f12daa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c68346c9-4d39-4898-a270-76d5f1f12daa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"UiqRT9Vxb76V"},"source":["4\\. Each e-mail has additionally been manually labeled as spam (1) or not (0). Find the TF-IDF representation of the bodies of the e-mails. Which e-mail was most similar to e-mail 0 (not spam)? Which e-mail was most similar to e-mail 1001 (spam)?"]},{"cell_type":"markdown","metadata":{"id":"Iy7hIjLDb76W"},"source":["5\\. (This exercise requires material from Part II of this book.) Write a function `predict_spam()` that accepts an e-mail body and predicts whether or not it is spam using $k$-nearest neighbors on the Enron spam data set. Use cosine distance ($= 1 - \\text{cosine similarity}$) as your distance metric.\n","\n","Use your model to predict whether an e-mail with the body \"free cash\" is spam or not."]}]}